{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dominant-capital",
   "metadata": {},
   "source": [
    "## Intro to Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63c0bb23-2d12-44a1-98aa-d4df4965d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you are starting from this notebook, please ensure you have uncommented and run:\n",
    "#If you need help running the command, please visit notebook \"1_CML_Session_Basics.ipynb\"\n",
    "#!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dense-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mineral-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "disturbed-beginning",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'REGION'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_219/2562446840.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"IntroToSparkSQL\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.hadoop.fs.s3a.s3guard.ddb.region\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"REGION\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.yarn.access.hadoopFileSystems\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"STORAGE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'REGION'"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"IntroToSparkSQL\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\", os.environ[\"REGION\"])\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\", os.environ[\"STORAGE\"])\\\n",
    "    .getOrCreate()\n",
    "#.config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-2\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "increasing-blackjack",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/home/cdsw/data/LoanStats_2015_subset.csv;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o63.csv.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/home/cdsw/data/LoanStats_2015_subset.csv;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:225)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:213)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:621)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a0124c25ab67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/LoanStats_2015_subset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/home/cdsw/data/LoanStats_2015_subset.csv;'"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data/LoanStats_2015_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing number of rows and columns:\n",
    "print('Dataframe Shape')\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-thumb",
   "metadata": {},
   "source": [
    "#### Basic Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count number of nulls for each column:\n",
    "df.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in df.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It seems like some columns have a lot of nulls while others have very few:\n",
    "nulls = df.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in df.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cols = nulls.T[(nulls.T > 1000).any(axis=1)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*null_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing number of rows:\n",
    "print('Dataframe Shape')\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.loan_status != '10500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-peoples",
   "metadata": {},
   "source": [
    "## KPI Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-relay",
   "metadata": {},
   "source": [
    "### What is the target variable and what does it define?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"loan_status\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Types of loan status\n",
    "print(df.groupBy('loan_status').count().show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-captain",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df.groupBy('loan_status').count().toPandas()\n",
    "plt.figure(figsize=(8,3))\n",
    "g = sns.barplot(x=\"loan_status\", y=\"count\", data=df_plot)\n",
    "g.set_title('Loan Status Category Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-limitation",
   "metadata": {},
   "source": [
    "#### To predict defaults, we need to transform the target variable into a binary variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-inquiry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"is_default\", when((df[\"loan_status\"] == \"Charged Off\")|(df[\"loan_status\"] == \"Default\"), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking that we have correctly replaced values\n",
    "df.select(\"is_default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"is_default\").dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the exact total of all loans labeled to default matches with the sum of the original two values used above (Charged Off and Default)\n",
    "df.select(F.sum(\"is_default\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-episode",
   "metadata": {},
   "source": [
    "### What is the monthly total loan volume in dollars and what is the monthly average loan size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The original issue date attribute\n",
    "df.select(\"issue_d\").show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to cast the issue date from string to month (all loan applications in the dataset occurred in 2015 so we don't need the year):\n",
    "df.selectExpr(\"from_unixtime(unix_timestamp(issue_d,'MMM-yyyy'),'MM') as issue_month\").show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"issue_month\",F.from_unixtime(F.unix_timestamp(F.col(\"issue_d\"),'MMM-yyyy'),'MM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"issue_month\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many loans defaulted for each month (all data is 2015):\n",
    "df.groupby('issue_month').sum('is_default').na.drop().sort(F.asc('issue_month')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults_date = df.groupby('issue_month').sum('is_default').na.drop().sort(F.asc('issue_month')).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "g = sns.barplot(x=\"issue_month\", y=\"sum(is_default)\", data=defaults_date)\n",
    "g.set_title('Loan Defaults by Month')\n",
    "g.set_ylabel('Total Loan Defaults')\n",
    "g.set_xlabel('Month in 2015')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create more plots here. First we aggregate in different ways. Then we join, convert to Pandas df, and plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "#by using like function\n",
    "df.groupBy(\"issue_month\",\"loan_status\").\\\n",
    "count().\\\n",
    "filter(F.lower(F.col(\"loan_status\")).like(\"late%\")).\\\n",
    "groupby('issue_month').\\\n",
    "sum().\\\n",
    "sort(F.asc('issue_month')).\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_late = df.groupBy(\"issue_month\",\"loan_status\").\\\n",
    "count().\\\n",
    "filter(F.lower(F.col(\"loan_status\")).like(\"late%\")).\\\n",
    "groupby('issue_month').\\\n",
    "sum().\\\n",
    "sort(F.asc('issue_month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "#by using like function\n",
    "df_delinq = df.groupBy(\"issue_month\").\\\n",
    "max(\"inq_last_6mths\").\\\n",
    "na.drop().\\\n",
    "sort(F.asc('issue_month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-advocate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This time we need to cast the attribute we are working with to numeric before we can create a similar dataframe:\n",
    "df = df.withColumn('loan_amnt', F.col('loan_amnt').cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "#by using like function\n",
    "df_ann_inc = df.groupBy(\"issue_month\").\\\n",
    "mean(\"loan_amnt\").\\\n",
    "na.drop().\\\n",
    "sort(F.asc('issue_month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-exemption",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delinq.alias('a').join(df_ann_inc.alias('b'),F.col('b.issue_month') == F.col('a.issue_month')).\\\n",
    "join(df_late.alias('c'), F.col('b.issue_month') == F.col('c.issue_month')).\\\n",
    "select(F.col('a.issue_month'), F.col('a.max(inq_last_6mths)'), F.col('b.avg(loan_amnt)'), F.col('c.sum(count)').alias('default_count')).\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_pd = df_delinq.alias('a').join(df_ann_inc.alias('b'),F.col('b.issue_month') == F.col('a.issue_month')).\\\n",
    "join(df_late.alias('c'), F.col('b.issue_month') == F.col('c.issue_month')).\\\n",
    "select(F.col('a.issue_month'), F.col('a.max(inq_last_6mths)'), F.col('b.avg(loan_amnt)'), F.col('c.sum(count)').alias('default_count')).\\\n",
    "toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(7, 5), sharex=True)\n",
    "\n",
    "sns.barplot(x=df_stats_pd['issue_month'], y=df_stats_pd['max(inq_last_6mths)'], palette=\"rocket\", ax=ax1)\n",
    "ax1.axhline(0, color=\"k\", clip_on=False)\n",
    "ax1.set_ylabel(\"Max Months Since Deliq\")\n",
    "ax1.set_xlabel(\"\")\n",
    "\n",
    "sns.scatterplot(x=df_stats_pd['issue_month'], y=df_stats_pd['avg(loan_amnt)'], palette=\"vlag\", ax=ax2)\n",
    "ax2.axhline(0, color=\"k\", clip_on=False)\n",
    "ax2.set_ylabel(\"Average Loan Amount\")\n",
    "\n",
    "sns.barplot(x=df_stats_pd['issue_month'], y=df_stats_pd['default_count'], palette=\"deep\", ax=ax3)\n",
    "ax3.axhline(0, color=\"k\", clip_on=False)\n",
    "ax3.set_ylabel(\"Count of Defaults\")\n",
    "ax1.set_xlabel(\"Month\")\n",
    "\n",
    "sns.despine(bottom=True)\n",
    "#plt.setp(f.axes)\n",
    "plt.tight_layout(h_pad=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do map based on zipcode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.filter(df.loan_status != '10500')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-parcel",
   "metadata": {},
   "source": [
    "### Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Registering the dataframe as a temporary table:\n",
    "#df.registerTempTable(\"LC_Loans_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "surprising-communications",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      databaseName|\n",
      "+------------------+\n",
      "|        big12stats|\n",
      "|           default|\n",
      "|          finance2|\n",
      "|           flights|\n",
      "|           indexed|\n",
      "|information_schema|\n",
      "|          omop_cdm|\n",
      "|  omop_cdm_parquet|\n",
      "|   prescribing_dev|\n",
      "|     prescribing_o|\n",
      "|     prescribing_p|\n",
      "|   prescribing_p_e|\n",
      "|        retaildemo|\n",
      "|               sys|\n",
      "|              test|\n",
      "|       ukcrime_dev|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "competent-myanmar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| default|             concept|      false|\n",
      "| default|     concept_synonym|      false|\n",
      "| default|     device_exposure|      false|\n",
      "| default|       drug_strength|      false|\n",
      "| default|   druide_kafka_demo|      false|\n",
      "| default|flight_not_partit...|      false|\n",
      "| default|  flight_partitioned|      false|\n",
      "| default|      lc_predictions|      false|\n",
      "| default|lc_predictions_la...|      false|\n",
      "| default|            lc_smote|      false|\n",
      "| default|   lc_smote_complete|      false|\n",
      "| default|         lc_smote_k2|      false|\n",
      "| default|         lc_smote_k3|      false|\n",
      "| default|         lc_smote_k4|      false|\n",
      "| default|         lc_smote_k5|      false|\n",
      "| default|            location|      false|\n",
      "| default|            metadata|      false|\n",
      "| default|              myview|      false|\n",
      "| default|             myview2|      false|\n",
      "| default|  observation_period|      false|\n",
      "+--------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "physical-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looks like revol_bal, tax_liens and tot_cur_bal should be numeric. Revol_util should also be numeric but we'll have to remove the % character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.write.format('parquet').mode(\"overwrite\").saveAsTable('default.LC_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running SQL like queries on the dataframe \n",
    "group_by_grade = spark.sql(\"SELECT grade, MEAN(loan_amnt) FROM LC_table WHERE grade IS NOT NULL GROUP BY grade ORDER BY grade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_grade.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming to pandas\n",
    "group_by_grade_pd = group_by_grade.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group_by_grade_pd.set_index('grade', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_subgrade = spark.sql(\"SELECT sub_grade, MEAN(loan_amnt), MEAN(annual_inc), SUM(is_default) FROM LC_table GROUP BY sub_grade ORDER BY sub_grade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache what you are going to use across queries (and early and often up to available memory)\n",
    "group_by_subgrade.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time group_by_grade.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time group_by_subgrade.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "#caching should reduce loading time for smaller dataframe -- check \n",
    "group_by_subgrade.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time group_by_subgrade.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_subgrade_pd = group_by_subgrade.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group_by_subgrade_pd = group_by_subgrade_pd.rename(columns={'avg(CAST(funded_amnt AS DOUBLE))':'avg(funded_amnt)'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_grade_pd.plot(kind='bar', figsize=(4,2))\n",
    "plt.title('Avg Loan Amount by Grade')\n",
    "plt.gca().legend_.remove()\n",
    "plt.show()\n",
    "#adjust styling here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(7, 5), sharex=True)\n",
    "\n",
    "sns.barplot(x=group_by_subgrade_pd['sub_grade'], y=group_by_subgrade_pd['avg(annual_inc)'], palette=\"hls\", ax=ax1)\n",
    "ax1.axhline(0, color=\"k\", clip_on=False)\n",
    "ax1.set_ylabel(\"Mean Annual Income\")\n",
    "ax1.set_xlabel(\"\")\n",
    "\n",
    "sns.barplot(x=group_by_subgrade_pd['sub_grade'], y=group_by_subgrade_pd['avg(loan_amnt)'], palette=\"vlag\", ax=ax2)\n",
    "ax2.axhline(0, color=\"k\", clip_on=False)\n",
    "ax2.set_ylabel(\"Mean Requested Amnt\")\n",
    "ax1.set_xlabel(\"\")\n",
    "\n",
    "sns.barplot(x=group_by_subgrade_pd['sub_grade'], y=group_by_subgrade_pd['sum(is_default)'], palette=\"rocket\", ax=ax3)\n",
    "ax3.axhline(0, color=\"k\", clip_on=False)\n",
    "ax3.set_ylabel(\"Total Number of Defaults\")\n",
    "\n",
    "#sns.despine(bottom=True)\n",
    "#plt.setp(f.axes)\n",
    "#plt.tight_layout(h_pad=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing from cache\n",
    "group_by_grade.unpersist()\n",
    "group_by_subgrade.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-acoustic",
   "metadata": {},
   "source": [
    "Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check dataframe columnns\n",
    "#df_new.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-scottish",
   "metadata": {},
   "source": [
    "Checking that correct data types were inferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following attributes are strings but they potentially should be numeric. Let's take a look at a sample.\n",
    "df.select('revol_bal', 'revol_util', 'tax_liens', 'tot_cur_bal', 'int_rate', 'emp_length').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import substring\n",
    "#df_ml = df_ml.withColumn(\"manufacturer\", substring(col(\"manufacturer\"), 0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#telco_data\\\n",
    "#  .write.format(\"parquet\")\\\n",
    "#  .mode(\"overwrite\")\\\n",
    "#  .saveAsTable(\n",
    "#    'default.telco_churn'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the % character from revol_util\n",
    "df = df.withColumn(\"revol_util\", F.expr(\"substring(revol_util, 1, length(revol_util)-1)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the % character from revol_util\n",
    "df = df.withColumn(\"int_rate\", F.expr(\"substring(int_rate, 1, length(revol_util)-1)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice that we are not casting emp_length to numeric as the time periods it represents are different - it will have to be one hot encoded\n",
    "integer = [\"revol_bal\", \"tax_liens\", \"tot_cur_bal\", \"funded_amnt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in integer:\n",
    "    df = df.withColumn(c, df[c].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating revol_util to double:\n",
    "df = df.withColumn('revol_util', F.col('revol_util').cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating int_rate to double:\n",
    "df = df.withColumn('int_rate', F.col('int_rate').cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in integer:\n",
    "    df = df.withColumn(c, F.col(c).cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Registering the dataframe as a temporary table:\n",
    "#Temporary views in Spark SQL are session-scoped and will disappear if the session that creates it terminates. \n",
    "#If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, \n",
    "#you can create a global temporary view\n",
    "\n",
    "df.createOrReplaceTempView(\"LC_Glob_Temp_View\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropTempView(\"LC_Glob_Temp_View\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.catalog.dropGlobalTempView(\"LC_Loans_2015_GlobalTempView\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM default.LC_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-metropolitan",
   "metadata": {},
   "source": [
    "Congratulations! You have learned a lot about the Spark SQL API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-easter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
