{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d38766ac-5e4c-47cb-b450-624bf8971aba",
   "metadata": {},
   "source": [
    "## How to Easily Write a CSV File to the Data Lake with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2294ded0-c1bd-49fa-83d4-59a46aab3d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you are starting from this notebook, please ensure you have uncommented and run:\n",
    "#If you need help running the command, please visit notebook \"1_CML_Session_Basics.ipynb\"\n",
    "#!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de7c4e0-9d0f-4fb0-aa5d-03836adf2c4a",
   "metadata": {},
   "source": [
    "#### This notebook showcases how to write a CSV file into a Hive or Imapala table with PySpark. There are three simple steps:\n",
    "##### 1. Save Storage Environment Variable (One Off)\n",
    "##### 2. Read the CSV file from a local CML Project Folder into Pandas\n",
    "##### 3. Transform the Pandas Dataframe to a Spark Dataframe and write that as a Hive table to the Data Lake\n",
    "##### 4. Optional: Validate in CDW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbabad6-cd4b-45df-8957-da77476c02ab",
   "metadata": {},
   "source": [
    "### Step 1: Save Storage Environment Variable "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b232898-7311-4ee9-8991-8902392a60ea",
   "metadata": {},
   "source": [
    "This step only needs to be executed once for the CML Project. The storage variable is saved as a CML Prokect Environment Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a505616e-d76e-425e-aa1f-9b622af256ab",
   "metadata": {},
   "source": [
    "Optionally, you could save this in a script and build automation with [APIv2](https://github.com/pdefusco/CML_AMP_APIv2) to execute it upon project creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66276d0e-f585-4662-8923-7903e8cd3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmlbootstrap import CMLBootstrap\n",
    "from IPython.display import Javascript, HTML\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import datetime\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "526cd33f-1f49-432f-83a1-5cdfe8a71bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_storage():\n",
    "    run_time_suffix = datetime.datetime.now()\n",
    "    run_time_suffix = run_time_suffix.strftime(\"%d%m%Y%H%M%S\")\n",
    "\n",
    "    # Set the setup variables needed by CMLBootstrap\n",
    "    HOST = os.getenv(\"CDSW_API_URL\").split(\":\")[0] + \"://\" + os.getenv(\"CDSW_DOMAIN\")\n",
    "    USERNAME = os.getenv(\"CDSW_PROJECT_URL\").split(\"/\")[6]  # args.username  # \"vdibia\"\n",
    "    API_KEY = os.getenv(\"CDSW_API_KEY\")\n",
    "    PROJECT_NAME = os.getenv(\"CDSW_PROJECT\")\n",
    "\n",
    "    # Instantiate API Wrapper\n",
    "    cml = CMLBootstrap(HOST, USERNAME, API_KEY, PROJECT_NAME)\n",
    "\n",
    "    # Set the STORAGE environment variable\n",
    "    try:\n",
    "        storage = os.environ[\"STORAGE\"]\n",
    "    except:\n",
    "        if os.path.exists(\"/etc/hadoop/conf/hive-site.xml\"):\n",
    "            tree = ET.parse(\"/etc/hadoop/conf/hive-site.xml\")\n",
    "            root = tree.getroot()\n",
    "            for prop in root.findall(\"property\"):\n",
    "                if prop.find(\"name\").text == \"hive.metastore.warehouse.dir\":\n",
    "                    storage = (\n",
    "                        prop.find(\"value\").text.split(\"/\")[0]\n",
    "                        + \"//\"\n",
    "                        + prop.find(\"value\").text.split(\"/\")[2]\n",
    "                    )\n",
    "        else:\n",
    "            storage = \"/user/\" + os.getenv(\"HADOOP_USER_NAME\")\n",
    "        storage_environment_params = {\"STORAGE\": storage}\n",
    "        storage_environment = cml.create_environment_variable(storage_environment_params)\n",
    "        os.environ[\"STORAGE\"] = storage\n",
    "    print(\"Storage Var Was Saved Permanently to the Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "667ae9f0-7774-4ee8-8cd2-79ae27111145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storage Var Was Saved Permanently to the Project\n"
     ]
    }
   ],
   "source": [
    "set_storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a33639-b276-4847-8261-4e2db4079b8f",
   "metadata": {},
   "source": [
    "### Step 2: Read the CSV file from a local CML Project Folder into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "501c7d9d-945f-4fd3-b63e-368eb62e91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5436aa6f-118e-4911-8920-a7f7a8885e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/LoanStats_2015_subset_071821.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e707fb5-55ed-4cad-b941-b25085484203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas Dataframe Shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18656, 79)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Pandas Dataframe Shape\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8881f1d7-3e6a-4fff-b790-4e37c28086bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all_util                               float64\n",
       "annual_inc                             float64\n",
       "annual_inc_joint                       float64\n",
       "bc_open_to_buy                         float64\n",
       "bc_util                                float64\n",
       "                                        ...   \n",
       "sec_app_open_act_il                    float64\n",
       "sec_app_num_rev_accts                  float64\n",
       "sec_app_chargeoff_within_12_mths       float64\n",
       "sec_app_collections_12_mths_ex_med     float64\n",
       "sec_app_mths_since_last_major_derog    float64\n",
       "Length: 79, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0f1ddb-25b0-438f-ae4c-b95834361041",
   "metadata": {},
   "source": [
    "### Step 3: Transform the Pandas Dataframe to a Spark Dataframe and write that as a Hive table to the Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2727aab0-a5d3-4c55-ae8d-421e16ed710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f237142a-6cd2-47f2-866a-7b77ea4537eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"PythonSQL\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\", \"us-east-2\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\", os.environ[\"STORAGE\"])\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e397e671-0dba-41a3-ad8a-491601471a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Pandas DF to Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae0c25dc-264b-42f9-80a7-cc1b4f97edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDf = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dca3b644-e477-4b8a-924c-9fd094bb8b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = 9faa6309-892f-4773-98f0-658a6fdc4b96\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sparkDf.write.format('parquet').mode(\"overwrite\").saveAsTable('default.my_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83a79117-7255-4166-b4b1-ac43bbb8a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07047d23-d665-41a0-a4bf-6cd78ef7c819",
   "metadata": {},
   "source": [
    "### Step 4: Verify Table in CDW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345263f0-8ee9-43cc-b93e-82d87e97db6e",
   "metadata": {},
   "source": [
    "##### Navigate to CDW from the CDP Home Page. Open the CDW Virtual Warehouse associated with the same Data Lake as this Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b2b6b-4850-462d-846d-24a20f444ea6",
   "metadata": {},
   "source": [
    "![alt text](img/cml_howo_7A.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7ea490-e952-4871-8296-f3b09c0a8ef0",
   "metadata": {},
   "source": [
    "##### Query the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac259eda-2240-440c-8c97-e6465f38183a",
   "metadata": {},
   "source": [
    "![alt text](img/cml_howto_7B.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ced88ac-e826-4783-9c27-e2e1fe8e657c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
